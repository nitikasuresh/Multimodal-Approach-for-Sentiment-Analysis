{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b6b839-c06d-449e-ba26-0b0e331f2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6974cb5a-10a3-429d-adb7-ef9e6643927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saisi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a350c3c-dce4-4895-9024-df21fc37c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input - Utterance or Column '1'\n",
    "# Emotion(7) - Output 1 and Column '3'\n",
    "# Sentiment(3) - Output 2 and Column '4'\n",
    "df1 = pd.read_csv('data1.csv')\n",
    "df2 = pd.read_csv('data2.csv')\n",
    "df3 = pd.read_csv('data3.csv')\n",
    "df4 = pd.read_csv('data4.csv')\n",
    "\n",
    "frames = [df1,df2,df3,df4]\n",
    "data = pd.concat(frames) #complete dataset\n",
    "\n",
    "mat =np.asarray([]) #emo\n",
    "mat1 =np.asarray([]) #senti\n",
    "data = np.asarray(data[['1','3','4']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda01545-4294-4cf0-bcc3-667e60f9ba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word vectors: 684830\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[:,0], data[:,1:3], test_size=0.2, shuffle = True)\n",
    "\n",
    "X_train_copy1 = X_train.copy()\n",
    "X_test_copy1 = X_test.copy()\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "print('Total word vectors:', total_vectors)\n",
    "\n",
    "y_train_emo = y_train[:,0] *1.0\n",
    "y_train_emo = y_train_emo.astype('int')\n",
    "y_test_emo = y_test[:,0] *1.0\n",
    "y_test_emo = y_test_emo.astype('int')\n",
    "\n",
    "y_train_senti = y_train[:,1] *1.0\n",
    "y_train_senti = y_train_senti.astype('int')\n",
    "y_test_senti = y_test[:,1] *1.0\n",
    "y_test_senti = y_test_senti.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f8e4104-66cf-4ce1-a0d0-e866002b3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in X_train_copy1])\n",
    "X_train_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_train_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n",
    "doc_glove_vectors2 = np.array([nlp(str(doc)).vector for doc in X_test_copy1])\n",
    "X_test_glove = np.zeros((doc_glove_vectors2.shape[0], 300))\n",
    "for i in range(doc_glove_vectors2.shape[0]):\n",
    "    if (doc_glove_vectors2[i].shape[0] == 300):\n",
    "        X_test_glove[i,:] = doc_glove_vectors2[i][:]\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f7f1db-ef73-460e-91d7-f787260c29c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n",
      "Training Time Taken : 6.601466178894043\n",
      "Training Accuracy :  0.5872702196139952\n",
      "Testing Accuracy :  0.5816979777432093\n",
      "Testing Time Taken : 0.0010027885437011719\n",
      "[[1243   28    2    2    9    2   11]\n",
      " [ 148  143    1    2    7    0   15]\n",
      " [  69    8    4    0    1    0    6]\n",
      " [ 166    5    0    5    4    1   10]\n",
      " [ 295   46    1    1   81    0   37]\n",
      " [  53    9    0    2    1    4    4]\n",
      " [ 206   37    2    1    3    1   66]]\n",
      "F1-Score :  0.5638220277169949\n",
      "(0.5872702196139952, 0.005041084284324297, 0.5816979777432093, 0.015162876329489549, 0.5638220277169949, 0.010001897811889648)\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Logistic Regression\n",
      "Training Time Taken : 4.136110305786133\n",
      "Training Accuracy :  0.6386091737154891\n",
      "Testing Accuracy :  0.6298259609376039\n",
      "Testing Time Taken : 0.0010001659393310547\n",
      "[[1237    8   52]\n",
      " [ 353   72  185]\n",
      " [ 495   15  325]]\n",
      "F1-Score :  0.5959153902261123\n",
      "(0.6386091737154891, 0.006236312431957808, 0.6298259609376039, 0.01291826204350013, 0.5959153902261123, 0.0029914379119873047)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter = 1e4)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    print(\"Training Time Taken :\", (time.time()-start))\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    start = time.time()\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    print(\"Testing Time Taken :\", (time.time()-start))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    \n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(scaler.fit_transform(X_train_glove), y_train_emo, scaler.fit_transform(X_test_glove), y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "print(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(scaler.fit_transform(X_train_glove), y_train_senti, scaler.fit_transform(X_test_glove), y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "print(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a25d34-7b84-41c4-a5f3-63cde8d22322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Naive Bayes\n",
      "Training Time Taken : 0.011192798614501953\n",
      "Training Accuracy :  0.49261344104324645\n",
      "Testing Time Taken : 0.0019025802612304688\n",
      "Testing Accuracy :  0.5051015117067528\n",
      "[[1277    7    0    0   13    0    0]\n",
      " [ 265   18    0    0   33    0    0]\n",
      " [  85    0    0    0    3    0    0]\n",
      " [ 187    0    0    0    4    0    0]\n",
      " [ 390    2    0    0   69    0    0]\n",
      " [  69    0    0    0    4    0    0]\n",
      " [ 293    2    0    0   21    0    0]]\n",
      "Time Taken : 0.03116631507873535\n",
      "F1-Score :  0.4974471188913202\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Naive Bayes\n",
      "Training Time Taken : 0.00904989242553711\n",
      "Training Accuracy :  0.5179640531882126\n",
      "Testing Time Taken : 0.0010023117065429688\n",
      "Testing Accuracy :  0.5415666174730432\n",
      "[[1279   14    4]\n",
      " [ 482   99   29]\n",
      " [ 736   56   43]]\n",
      "Time Taken : 0.030188798904418945\n",
      "F1-Score :  0.5182348650619986\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "def naive_bayes(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Multinomial Naive Bayes\")\n",
    "    classifier = MultinomialNB();\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"Training Time Taken :\", (time.time()-start))\n",
    "    accuracy_train_data = cross_val_score(classifier, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    start = time.time()\n",
    "    # predicting test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(\"Testing Time Taken :\", (time.time()-start))\n",
    "    accuracy_test_data = cross_val_score(classifier, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    \n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = naive_bayes(scaler.fit_transform(X_train_glove), y_train_emo, scaler.fit_transform(X_test_glove), y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = naive_bayes(scaler.fit_transform(X_train_glove), y_train_senti, scaler.fit_transform(X_test_glove), y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72255204-c0f8-4b44-8ee0-6922a3ba790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Random Forest Classifier\n",
      "Training Time Taken : 11.584264516830444\n",
      "Training Accuracy :  0.5688485489381354\n",
      "Testing Time Taken : 0.0631096363067627\n",
      "Testing Accuracy :  0.5579846569077154\n",
      "[[1237   18    0    1   36    0    5]\n",
      " [ 133  115    1    1   45    0   21]\n",
      " [  74    6    2    0    4    0    2]\n",
      " [ 159    3    1    8   14    0    6]\n",
      " [ 235   14    0    1  193    0   18]\n",
      " [  58    4    0    0    8    1    2]\n",
      " [ 185   18    0    0   67    1   45]]\n",
      "F1-Score :  0.5838803792851933\n",
      "Time Taken : 8.933932781219482\n",
      "(0.5688485489381354, 0.010273051841479384, 0.5579846569077154, 0.013838154323486403, 0.5838803792851933, 8.933932781219482)\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Random Forest Classifier\n",
      "Training Time Taken : 10.305659770965576\n",
      "Training Accuracy :  0.6273022931586829\n",
      "Testing Time Taken : 0.06807684898376465\n",
      "Testing Accuracy :  0.61158908699294\n",
      "[[1182   34   81]\n",
      " [ 237  232  141]\n",
      " [ 409   92  334]]\n",
      "F1-Score :  0.637490882567469\n",
      "Time Taken : 8.030803918838501\n",
      "(0.6273022931586829, 0.00885629150983034, 0.61158908699294, 0.01877329590282401, 0.637490882567469, 8.030803918838501)\n"
     ]
    }
   ],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Random Forest Classifier\")\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print(\"Training Time Taken :\", (time.time()-start))\n",
    "    accuracy_train_data = cross_val_score(rf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "\n",
    "    # predicting test set results\n",
    "    start = time.time()\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(\"Testing Time Taken :\", (time.time()-start))\n",
    "    accuracy_test_data = cross_val_score(rf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = random_forest(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "print(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup= random_forest(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)\n",
    "print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb897751-443b-46d9-b820-1fe02f0895ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Support Vector Classifier\n",
      "Training Time Taken : 19.9677836894989\n",
      "Training Accuracy :  0.6022250709870347\n",
      "Testing Time Taken : 6.881994247436523\n",
      "Testing Accuracy :  0.5875287516785661\n",
      "[[1226   14    1    2   45    0    9]\n",
      " [ 120  135    1    0   35    0   25]\n",
      " [  61    7    2    2    2    0   14]\n",
      " [ 150    4    0   12   13    0   12]\n",
      " [ 190   13    0    1  228    0   29]\n",
      " [  55    5    0    1    3    1    8]\n",
      " [ 142   25    0    0   55    0   94]]\n",
      "F1-Score :  0.6192560175054704\n",
      "Time Taken : 10.940051317214966\n",
      "(0.6022250709870347, 0.007349465033742871, 0.5875287516785661, 0.013113171581575966, 0.6192560175054704, 10.940051317214966)\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Support Vector Classifier\n",
      "Training Time Taken : 19.909106492996216\n",
      "Training Accuracy :  0.6620465132906102\n",
      "Testing Time Taken : 6.171229600906372\n",
      "Testing Accuracy :  0.6389447302992833\n",
      "[[1177   44   76]\n",
      " [ 199  276  135]\n",
      " [ 362   81  392]]\n",
      "F1-Score :  0.6728665207877462\n",
      "Time Taken : 9.96702766418457\n",
      "(0.6620465132906102, 0.006408747077390795, 0.6389447302992833, 0.012352118644034259, 0.6728665207877462, 9.96702766418457)\n"
     ]
    }
   ],
   "source": [
    "def sv_classifier(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Support Vector Classifier\")\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Training Time Taken :\", (time.time()-start))\n",
    "    accuracy_train_data = cross_val_score(clf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    # predicting test set results\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Testing Time Taken :\", (time.time()-start))\n",
    "    accuracy_test_data = cross_val_score(clf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "print(tup)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "print(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "861c972c-56cd-457d-9433-c8552537cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_senti = np.zeros((y_train_senti.shape[0], 3))\n",
    "for i in range(y_train_senti.shape[0]):\n",
    "    if y_train_senti[i] == 0:\n",
    "        Y_train_senti[i,:] = [1, 0, 0]\n",
    "    if y_train_senti[i] == 1:\n",
    "        Y_train_senti[i,:] = [0, 1, 0]\n",
    "    if y_train_senti[i] == 2:\n",
    "        Y_train_senti[i,:] = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1388dbaa-c801-4b7c-9717-ed36e5ac5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_senti = np.zeros((y_test_senti.shape[0], 3))\n",
    "for i in range(y_test_senti.shape[0]):\n",
    "    if y_test_senti[i] == 0:\n",
    "        Y_test_senti[i,:] = [1, 0, 0]\n",
    "    if y_test_senti[i] == 1:\n",
    "        Y_test_senti[i,:] = [0, 1, 0]\n",
    "    if y_test_senti[i] == 2:\n",
    "        Y_test_senti[i,:] = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ebab636-e856-4cb6-beca-a2db35f91e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2742, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_glove.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b27471a2-3f8b-4f48-b509-d784c928f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_x, train_y = X_train_glove.astype(np.float32), Y_train_senti.astype(np.int)\n",
    "valid_x, valid_y = X_test_glove.astype(np.float32), Y_test_senti.astype(np.int)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(300, 3000)\n",
    "        self.fc2 = nn.Linear(3000, 2000)\n",
    "        self.fc3 = nn.Linear(2000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 300)\n",
    "        self.fc6 = nn.Linear(300, 3)\n",
    "#         self.fc5 = nn.Linear(2000, 1000)\n",
    "#         self.fc6 = nn.Linear(1000, 500)\n",
    "#         self.fc7 = nn.Linear(500, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "\n",
    "givendata_train = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "givendata_test = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "trainset_loader = DataLoader(givendata_train, batch_size=64, shuffle=True, num_workers=1)\n",
    "validset_loader = DataLoader(givendata_test, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "def train(max_iters):\n",
    "    model.train()\n",
    "    Taccuracies = []\n",
    "    Tlosses = []\n",
    "    for itr in range(max_iters):\n",
    "        correct = 0\n",
    "        Tloss = 0\n",
    "        num = 0\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target = torch.max(target, 1)[1]\n",
    "            loss = nn.functional.cross_entropy(output, target)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            Tloss = Tloss + loss.item()\n",
    "\n",
    "            pred = torch.max(output, 1)[1] \n",
    "            correct += pred.eq(target).sum().item()\n",
    "            num = num + 1\n",
    "        \n",
    "        Taccuracies.append(100. * correct / (num*64))\n",
    "        Tlosses.append(Tloss/num)\n",
    "        if itr % 50 == 0:\n",
    "            print('Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "            print('Loss: {:.6f}'.format(Tloss/num))\n",
    "            \n",
    "    return Taccuracies, Tlosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3370019a-2876-4e95-8881-c1111001d0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            903,000\n",
      "├─Linear: 1-2                            6,002,000\n",
      "├─Linear: 1-3                            2,001,000\n",
      "├─Linear: 1-4                            500,500\n",
      "├─Linear: 1-5                            150,300\n",
      "├─Linear: 1-6                            903\n",
      "=================================================================\n",
      "Total params: 9,557,703\n",
      "Trainable params: 9,557,703\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            903,000\n",
      "├─Linear: 1-2                            6,002,000\n",
      "├─Linear: 1-3                            2,001,000\n",
      "├─Linear: 1-4                            500,500\n",
      "├─Linear: 1-5                            150,300\n",
      "├─Linear: 1-6                            903\n",
      "=================================================================\n",
      "Total params: 9,557,703\n",
      "Trainable params: 9,557,703\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_x, train_y = X_train_glove.astype(np.float32), Y_train_senti.astype(np.int)\n",
    "valid_x, valid_y = X_test_glove.astype(np.float32), Y_test_senti.astype(np.int)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(300, 3000)\n",
    "        self.fc2 = nn.Linear(3000, 2000)\n",
    "        self.fc3 = nn.Linear(2000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 300)\n",
    "        self.fc6 = nn.Linear(300, 3)\n",
    "#         self.fc5 = nn.Linear(2000, 1000)\n",
    "#         self.fc6 = nn.Linear(1000, 500)\n",
    "#         self.fc7 = nn.Linear(500, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "print(summary(model))\n",
    "\n",
    "givendata_train = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "givendata_test = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "trainset_loader = DataLoader(givendata_train, batch_size=64, shuffle=True, num_workers=1)\n",
    "validset_loader = DataLoader(givendata_test, batch_size=64, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15d23dcf-7049-48d7-8190-d1576eb6f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 43.10 %\n",
      "Loss: 1.088555\n",
      "Accuracy 47.67 %\n",
      "Loss: 1.002067\n",
      "Accuracy 59.28 %\n",
      "Loss: 0.856523\n",
      "Accuracy 61.46 %\n",
      "Loss: 0.824653\n",
      "Accuracy 65.92 %\n",
      "Loss: 0.763544\n",
      "Accuracy 73.81 %\n",
      "Loss: 0.622689\n",
      "Training Time Taken : 2095.4107105731964\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "Taccuracies, Tlosses = train(300)\n",
    "print(\"Training Time Taken :\", (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d72f27d-8697-41ab-b4e0-5ba7e5db9a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 64.06 %\n",
      "Loss: 1.061929\n",
      "Testing Time Taken : 1.2749571800231934\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.eval()\n",
    "Taccuracies = []\n",
    "Tlosses = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    Tloss = 0\n",
    "    num = 0\n",
    "    for batch_idx, (data, target) in enumerate(validset_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        target = torch.max(target, 1)[1]\n",
    "        loss = nn.functional.cross_entropy(output, target)            \n",
    "        Tloss = Tloss + loss.item()\n",
    "\n",
    "        pred = torch.max(output, 1)[1] \n",
    "        correct += pred.eq(target).sum().item()\n",
    "        num = num + 1\n",
    "\n",
    "    Taccuracies.append(100. * correct / (num*64))\n",
    "    Tlosses.append(Tloss/num)\n",
    "    print('Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "    print('Loss: {:.6f}'.format(Tloss/num))\n",
    "print(\"Testing Time Taken :\", (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ade6945c-a965-4ca1-8204-9442047de698",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_emo = np.zeros((y_train_emo.shape[0], 7))\n",
    "for i in range(y_train_emo.shape[0]):\n",
    "    if y_train_emo[i] == 0:\n",
    "        Y_train_emo[i,:] = [1, 0, 0, 0, 0, 0, 0]\n",
    "    if y_train_emo[i] == 1:\n",
    "        Y_train_emo[i,:] = [0, 1, 0, 0, 0, 0, 0]\n",
    "    if y_train_emo[i] == 2:\n",
    "        Y_train_emo[i,:] = [0, 0, 1, 0, 0, 0, 0]\n",
    "    if y_train_emo[i] == 3:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 1, 0, 0, 0]\n",
    "    if y_train_emo[i] == 4:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 0, 1, 0, 0]\n",
    "    if y_train_emo[i] == 5:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 0, 0, 1, 0]\n",
    "    if y_train_emo[i] == 6:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 0, 0, 0, 1]\n",
    "        \n",
    "        \n",
    "Y_test_emo = np.zeros((y_test_emo.shape[0], 7))\n",
    "for i in range(y_test_emo.shape[0]):\n",
    "    if y_test_emo[i] == 0:\n",
    "        Y_test_emo[i,:] = [1, 0, 0, 0, 0, 0, 0]\n",
    "    if y_test_emo[i] == 1:\n",
    "        Y_test_emo[i,:] = [0, 1, 0, 0, 0, 0, 0]\n",
    "    if y_test_emo[i] == 2:\n",
    "        Y_test_emo[i,:] = [0, 0, 1, 0, 0, 0, 0]\n",
    "    if y_test_emo[i] == 3:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 1, 0, 0, 0]\n",
    "    if y_test_emo[i] == 4:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 0, 1, 0, 0]\n",
    "    if y_test_emo[i] == 5:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 0, 0, 1, 0]\n",
    "    if y_test_emo[i] == 6:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3305c5ed-a3f2-466a-b489-c10be9dfb6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_x, train_y = X_train_glove.astype(np.float32), Y_train_emo.astype(np.int)\n",
    "valid_x, valid_y = X_test_glove.astype(np.float32), Y_test_emo.astype(np.int)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(300, 3000)\n",
    "        self.fc2 = nn.Linear(3000, 2000)\n",
    "        self.fc3 = nn.Linear(2000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 300)\n",
    "        self.fc6 = nn.Linear(300, 7)\n",
    "#         self.fc5 = nn.Linear(2000, 1000)\n",
    "#         self.fc6 = nn.Linear(1000, 500)\n",
    "#         self.fc7 = nn.Linear(500, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "\n",
    "givendata_train = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "givendata_test = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "trainset_loader = DataLoader(givendata_train, batch_size=64, shuffle=True, num_workers=1)\n",
    "validset_loader = DataLoader(givendata_test, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "def train(max_iters):\n",
    "    model.train()\n",
    "    Taccuracies = []\n",
    "    Tlosses = []\n",
    "    for itr in range(max_iters):\n",
    "        correct = 0\n",
    "        Tloss = 0\n",
    "        num = 0\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target = torch.max(target, 1)[1]\n",
    "            loss = nn.functional.cross_entropy(output, target)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            Tloss = Tloss + loss.item()\n",
    "\n",
    "            pred = torch.max(output, 1)[1] \n",
    "            correct += pred.eq(target).sum().item()\n",
    "            num = num + 1\n",
    "        \n",
    "        Taccuracies.append(100. * correct / (num*64))\n",
    "        Tlosses.append(Tloss/num)\n",
    "        if itr % 50 == 0:\n",
    "            print('Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "            print('Loss: {:.6f}'.format(Tloss/num))\n",
    "            \n",
    "    return Taccuracies, Tlosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa24548b-4233-411c-8ce3-bac02f20113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 46.02 %\n",
      "Loss: 1.864360\n",
      "Accuracy 52.52 %\n",
      "Loss: 1.395798\n",
      "Accuracy 54.28 %\n",
      "Loss: 1.322285\n",
      "Accuracy 54.38 %\n",
      "Loss: 1.290819\n",
      "Accuracy 55.13 %\n",
      "Loss: 1.239628\n",
      "Accuracy 59.96 %\n",
      "Loss: 1.098693\n"
     ]
    }
   ],
   "source": [
    "Taccuracies, Tlosses = train(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45b78937-7515-46a4-bbca-664c4257d833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 54.98 %\n",
      "Loss: 1.431657\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "Taccuracies = []\n",
    "Tlosses = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    Tloss = 0\n",
    "    num = 0\n",
    "    for batch_idx, (data, target) in enumerate(validset_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        target = torch.max(target, 1)[1]\n",
    "        loss = nn.functional.cross_entropy(output, target)            \n",
    "        Tloss = Tloss + loss.item()\n",
    "\n",
    "        pred = torch.max(output, 1)[1] \n",
    "        correct += pred.eq(target).sum().item()\n",
    "        num = num + 1\n",
    "\n",
    "    Taccuracies.append(100. * correct / (num*64))\n",
    "    Tlosses.append(Tloss/num)\n",
    "    print('Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "    print('Loss: {:.6f}'.format(Tloss/num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402178b3-6fe6-498e-9039-e1364a57506a",
   "metadata": {},
   "source": [
    "### Feature Engineering ---- Seems useless for GLOVE vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9ffb4eb-820c-4b53-b81a-021656a65ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_remover(x):\n",
    "    X_t1 = x\n",
    "    for i in range(len(x)):\n",
    "        X_t1[i] = [char for char in x[i] if char not in string.punctuation]\n",
    "        X_t1[i]=''.join(X_t1[i])\n",
    "    return X_t1\n",
    "\n",
    "#All useless punctuations are removed\n",
    "X_tr1 = punctuation_remover(X_train_copy1)\n",
    "X_te1 = punctuation_remover(X_test_copy1)\n",
    "\n",
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in X_tr1])\n",
    "X_train_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_train_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n",
    "doc_glove_vectors2 = np.array([nlp(str(doc)).vector for doc in X_te1])\n",
    "X_test_glove = np.zeros((doc_glove_vectors2.shape[0], 300))\n",
    "for i in range(doc_glove_vectors2.shape[0]):\n",
    "    if (doc_glove_vectors2[i].shape[0] == 300):\n",
    "        X_test_glove[i,:] = doc_glove_vectors2[i][:]\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e44efc9b-b31c-45a0-93bd-cbc24f617577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5116734234767872\n",
      "Testing Accuracy :  0.4894207118450268\n",
      "[[1116   37    1   19   50    3   23]\n",
      " [ 196  110    0    9   14    0   15]\n",
      " [  54    3    2    3    1    0    4]\n",
      " [ 153    8    0   17    5    1   10]\n",
      " [ 366   13    1    6   92    1   12]\n",
      " [  58    6    0    2    0    6    7]\n",
      " [ 258   13    1    4   10    2   30]]\n",
      "Time Taken : 16.00754189491272\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5519780556431939\n",
      "Testing Accuracy :  0.5321001688537885\n",
      "[[975  96 178]\n",
      " [362 180 125]\n",
      " [433  44 349]]\n",
      "Time Taken : 9.973003625869751\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter = 1e4)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5556d38d-b823-4805-8990-db21111b3f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Random Forest Classifier\n",
      "Training Accuracy :  0.5100308805551433\n",
      "Testing Accuracy :  0.48067488333133895\n",
      "[[1208   12    2    2   20    0    5]\n",
      " [ 250   77    1    1    9    1    5]\n",
      " [  60    4    1    0    0    0    2]\n",
      " [ 180    5    1    6    1    1    0]\n",
      " [ 411    7    1    2   69    0    1]\n",
      " [  71    2    1    1    1    2    1]\n",
      " [ 295    5    0    1    3    0   14]]\n",
      "Time Taken : 151.55781745910645\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Random Forest Classifier\n",
      "Training Accuracy :  0.5430423145493596\n",
      "Testing Accuracy :  0.5120364830547911\n",
      "[[1092   37  120]\n",
      " [ 452  136   79]\n",
      " [ 544   22  260]]\n",
      "Time Taken : 130.1093897819519\n"
     ]
    }
   ],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Random Forest Classifier\")\n",
    "    rf = RandomForestClassifier(random_state=42, n_estimators  = 180)\n",
    "    rf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(rf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "\n",
    "    # predicting test set results\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(rf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = random_forest(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup= random_forest(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9467fec1-9db5-4919-81dd-ebbf3cf7bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.5186937304866192\n",
      "Testing Accuracy :  0.49599005491072023\n",
      "[[1218    9    0    3   17    0    2]\n",
      " [ 238   94    0    0    9    0    3]\n",
      " [  62    3    0    1    0    0    1]\n",
      " [ 171    8    0   13    1    1    0]\n",
      " [ 396    7    1    2   84    0    1]\n",
      " [  71    4    0    1    0    1    2]\n",
      " [ 294    6    0    1    6    0   11]]\n",
      "Time Taken : 128.2013020515442\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.561918027900991\n",
      "Testing Accuracy :  0.5466913964341271\n",
      "[[1068   50  131]\n",
      " [ 389  173  105]\n",
      " [ 460   27  339]]\n",
      "Time Taken : 115.29567837715149\n"
     ]
    }
   ],
   "source": [
    "def sv_classifier(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Support Vector Classifier\")\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(clf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    # predicting test set results\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(clf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b516267-7d3e-4db8-8b3e-d5fce82f2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = set(stopwords.words('english'))\n",
    "#print(StopWords)\n",
    "\n",
    "#train2 and test2 after stopwords removed \n",
    "def stopwords_remover(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = [word for word in x[i].split() if word.lower() not in StopWords]\n",
    "        \n",
    "    a = \"\"\n",
    "    for i in range(len(x)):\n",
    "        for word in x[i]:\n",
    "            #print(word)\n",
    "            a = a +word +\" \"\n",
    "        x[i]=a\n",
    "        a=\"\"\n",
    "    return x\n",
    "\n",
    "X_tr1 = stopwords_remover(X_train_copy1)\n",
    "X_te1 = stopwords_remover(X_test_copy1)\n",
    "\n",
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in X_tr1])\n",
    "X_train_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_train_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n",
    "doc_glove_vectors2 = np.array([nlp(str(doc)).vector for doc in X_te1])\n",
    "X_test_glove = np.zeros((doc_glove_vectors2.shape[0], 300))\n",
    "for i in range(doc_glove_vectors2.shape[0]):\n",
    "    if (doc_glove_vectors2[i].shape[0] == 300):\n",
    "        X_test_glove[i,:] = doc_glove_vectors2[i][:]\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60c97e2c-3182-4429-bb7a-22826e22bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5041042165737424\n",
      "Testing Accuracy :  0.45697419329105343\n",
      "[[1115   36    2   19   44    4   29]\n",
      " [ 213   81    1    8   18    1   22]\n",
      " [  56    3    0    3    0    0    5]\n",
      " [ 151    6    0   17    6    3   11]\n",
      " [ 365   15    1    7   96    1    6]\n",
      " [  55    6    1    3    0    7    7]\n",
      " [ 242   11    3    6   18    5   33]]\n",
      "Time Taken : 14.356157779693604\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.543223382927613\n",
      "Testing Accuracy :  0.5233523460040154\n",
      "[[1003   87  159]\n",
      " [ 386  183   98]\n",
      " [ 448   60  318]]\n",
      "Time Taken : 8.695868253707886\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter = 1e4)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4620ad8c-fc55-42f6-ae29-64e0d1a6e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def stem_sentences(x):\n",
    "    sen_list = []\n",
    "    for sentence in x:\n",
    "        tokens = sentence.split()\n",
    "#         print(tokens)\n",
    "        stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "#         print(stemmed_tokens)\n",
    "        stemmed_string =  ' '.join(stemmed_tokens)\n",
    "        sen_list.append(stemmed_string)\n",
    "    sen_list_arr = np.asarray(sen_list)\n",
    "    return sen_list_arr\n",
    "\n",
    "X_tr1 = stem_sentences(X_train_copy1)\n",
    "X_te1 = stem_sentences(X_test_copy1)\n",
    "\n",
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in X_tr1])\n",
    "X_train_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_train_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n",
    "doc_glove_vectors2 = np.array([nlp(str(doc)).vector for doc in X_te1])\n",
    "X_test_glove = np.zeros((doc_glove_vectors2.shape[0], 300))\n",
    "for i in range(doc_glove_vectors2.shape[0]):\n",
    "    if (doc_glove_vectors2[i].shape[0] == 300):\n",
    "        X_test_glove[i,:] = doc_glove_vectors2[i][:]\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da336fa8-9e8d-4fb4-ab33-9caedc6bdbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5017330771107706\n",
      "Testing Accuracy :  0.45369085131559705\n",
      "[[1112   28    3   22   54    4   26]\n",
      " [ 227   80    1    7   14    1   14]\n",
      " [  55    4    0    3    0    0    5]\n",
      " [ 157    7    1   16    6    5    2]\n",
      " [ 373   15    1    8   83    2    9]\n",
      " [  57    7    0    2    1    7    5]\n",
      " [ 261    8    2    5   15    2   25]]\n",
      "Time Taken : 13.163366556167603\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5399411652473416\n",
      "Testing Accuracy :  0.4817790807440203\n",
      "[[998  91 160]\n",
      " [400 176  91]\n",
      " [487  54 285]]\n",
      "Time Taken : 7.963275194168091\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter = 1e4)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "803647d2-0948-4743-8654-e5c53ff0eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr1 = stem_sentences(stopwords_remover(punctuation_remover(X_train_copy1)))\n",
    "X_te1 = stem_sentences(stopwords_remover(punctuation_remover(X_test_copy1)))\n",
    "\n",
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in X_tr1])\n",
    "X_train_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_train_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n",
    "doc_glove_vectors2 = np.array([nlp(str(doc)).vector for doc in X_te1])\n",
    "X_test_glove = np.zeros((doc_glove_vectors2.shape[0], 300))\n",
    "for i in range(doc_glove_vectors2.shape[0]):\n",
    "    if (doc_glove_vectors2[i].shape[0] == 300):\n",
    "        X_test_glove[i,:] = doc_glove_vectors2[i][:]\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1b0690f-acc7-42b7-bfcb-157f3ef22b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5017330771107706\n",
      "Testing Accuracy :  0.45369085131559705\n",
      "[[1112   28    3   22   54    4   26]\n",
      " [ 227   80    1    7   14    1   14]\n",
      " [  55    4    0    3    0    0    5]\n",
      " [ 157    7    1   16    6    5    2]\n",
      " [ 373   15    1    8   83    2    9]\n",
      " [  57    7    0    2    1    7    5]\n",
      " [ 261    8    2    5   15    2   25]]\n",
      "Time Taken : 13.201356887817383\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5399411652473416\n",
      "Testing Accuracy :  0.4817790807440203\n",
      "[[998  91 160]\n",
      " [400 176  91]\n",
      " [487  54 285]]\n",
      "Time Taken : 7.921770334243774\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter = 1e4)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2e728a6-3908-4a35-ac0e-90d378ff7f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.5169609859164883\n",
      "Testing Accuracy :  0.48067355377394866\n",
      "[[1206    7    1    7   22    1    5]\n",
      " [ 268   58    0    4    8    0    6]\n",
      " [  62    3    0    2    0    0    0]\n",
      " [ 168    6    0   14    3    3    0]\n",
      " [ 406    8    0    5   71    1    0]\n",
      " [  69    3    0    0    1    2    4]\n",
      " [ 291    7    0    1    8    1   10]]\n",
      "Time Taken : 133.3147828578949\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.550245976154342\n",
      "Testing Accuracy :  0.5215222102562057\n",
      "[[1077   41  131]\n",
      " [ 426  152   89]\n",
      " [ 520   31  275]]\n",
      "Time Taken : 104.24002289772034\n"
     ]
    }
   ],
   "source": [
    "def sv_classifier(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Support Vector Classifier\")\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(clf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    # predicting test set results\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(clf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85cd6e-3f12-4ec4-949a-d42cc36e4f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
