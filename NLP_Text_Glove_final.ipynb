{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b6b839-c06d-449e-ba26-0b0e331f2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6974cb5a-10a3-429d-adb7-ef9e6643927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saisi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a350c3c-dce4-4895-9024-df21fc37c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input - Utterance or Column '1'\n",
    "# Emotion(7) - Output 1 and Column '3'\n",
    "# Sentiment(3) - Output 2 and Column '4'\n",
    "df1 = pd.read_csv('data1.csv')\n",
    "df2 = pd.read_csv('data2.csv')\n",
    "df3 = pd.read_csv('data3.csv')\n",
    "df4 = pd.read_csv('data4.csv')\n",
    "\n",
    "frames = [df1,df2,df3,df4]\n",
    "data = pd.concat(frames) #complete dataset\n",
    "\n",
    "mat =np.asarray([]) #emo\n",
    "mat1 =np.asarray([]) #senti\n",
    "data = np.asarray(data[['1','3','4']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda01545-4294-4cf0-bcc3-667e60f9ba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word vectors: 684830\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[:,0], data[:,1:3], test_size=0.2, shuffle = True)\n",
    "\n",
    "X_train_copy1 = X_train.copy()\n",
    "X_test_copy1 = X_test.copy()\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "print('Total word vectors:', total_vectors)\n",
    "\n",
    "y_train_emo = y_train[:,0] *1.0\n",
    "y_train_emo = y_train_emo.astype('int')\n",
    "y_test_emo = y_test[:,0] *1.0\n",
    "y_test_emo = y_test_emo.astype('int')\n",
    "\n",
    "y_train_senti = y_train[:,1] *1.0\n",
    "y_train_senti = y_train_senti.astype('int')\n",
    "y_test_senti = y_test[:,1] *1.0\n",
    "y_test_senti = y_test_senti.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8e4104-66cf-4ce1-a0d0-e866002b3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in X_train_copy1])\n",
    "X_train_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_train_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n",
    "doc_glove_vectors2 = np.array([nlp(str(doc)).vector for doc in X_test_copy1])\n",
    "X_test_glove = np.zeros((doc_glove_vectors2.shape[0], 300))\n",
    "for i in range(doc_glove_vectors2.shape[0]):\n",
    "    if (doc_glove_vectors2[i].shape[0] == 300):\n",
    "        X_test_glove[i,:] = doc_glove_vectors2[i][:]\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4f7f1db-ef73-460e-91d7-f787260c29c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5920117503234997\n",
      "Testing Accuracy :  0.5594458404797042\n",
      "[[1005   38    3    2   31    3  172]\n",
      " [  94  130    2    0    4    0  108]\n",
      " [  40    3    6    0    1    0   23]\n",
      " [ 120    4    2    9    1    0   64]\n",
      " [ 188   18    0    0   74    0  217]\n",
      " [  29    3    0    1    1    4   37]\n",
      " [  81   16    3    0    3    0  202]]\n",
      "F1-Score :  0.5215171407731582\n",
      "Time Taken : 33.82595920562744\n",
      "(0.5920117503234997, 0.006148802035596928, 0.5594458404797042, 0.007143008751892763, 0.5215171407731582, 33.82595920562744)\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.6484584455138398\n",
      "Testing Accuracy :  0.6079454349647002\n",
      "[[914  31 309]\n",
      " [203 123 340]\n",
      " [229  12 581]]\n",
      "F1-Score :  0.5900802334062728\n",
      "Time Taken : 19.58925151824951\n",
      "(0.6484584455138398, 0.008410712348025163, 0.6079454349647002, 0.013639857048245019, 0.5900802334062728, 19.58925151824951)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter = 1e4)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(scaler.fit_transform(X_train_glove), y_train_emo, scaler.fit_transform(X_test_glove), y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "print(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(scaler.fit_transform(X_train_glove), y_train_senti, scaler.fit_transform(X_test_glove), y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "print(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7a25d34-7b84-41c4-a5f3-63cde8d22322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Naive Bayes\n",
      "Training Accuracy :  0.4969906734820871\n",
      "Testing Accuracy :  0.48576908247244494\n",
      "[[1219   12    0    0   23    0    0]\n",
      " [ 262   38    0    0   38    0    0]\n",
      " [  70    2    0    0    1    0    0]\n",
      " [ 196    1    0    0    3    0    0]\n",
      " [ 429    1    0    0   67    0    0]\n",
      " [  73    0    0    0    2    0    0]\n",
      " [ 287    1    0    0   17    0    0]]\n",
      "Time Taken : 0.10944962501525879\n",
      "F1-Score :  0.4828592268417214\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Naive Bayes\n",
      "Training Accuracy :  0.522980553439073\n",
      "Testing Accuracy :  0.5196887506149203\n",
      "[[1215   23   16]\n",
      " [ 513   98   55]\n",
      " [ 691   47   84]]\n",
      "Time Taken : 0.10347843170166016\n",
      "F1-Score :  0.5094821298322393\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "def naive_bayes(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Multinomial Naive Bayes\")\n",
    "    classifier = MultinomialNB();\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(classifier, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    # predicting test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(classifier, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    \n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = naive_bayes(scaler.fit_transform(X_train_glove), y_train_emo, scaler.fit_transform(X_test_glove), y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = naive_bayes(scaler.fit_transform(X_train_glove), y_train_senti, scaler.fit_transform(X_test_glove), y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72255204-c0f8-4b44-8ee0-6922a3ba790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Random Forest Classifier\n",
      "Training Accuracy :  0.5719501970511127\n",
      "Testing Accuracy :  0.5419402230997301\n",
      "[[1182   24    0    4   34    0   10]\n",
      " [ 166  117    0    0   44    0   11]\n",
      " [  57    4    1    0    6    0    5]\n",
      " [ 159    8    0    9   16    0    8]\n",
      " [ 290   11    0    0  181    1   14]\n",
      " [  54    3    0    0   10    0    8]\n",
      " [ 184   14    0    1   51    0   55]]\n",
      "F1-Score :  0.563457330415755\n",
      "Time Taken : 60.72662568092346\n",
      "(0.5719501970511127, 0.0061161177068875545, 0.5419402230997301, 0.006024677876830581, 0.563457330415755, 60.72662568092346)\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Random Forest Classifier\n",
      "Training Accuracy :  0.6346892677912359\n",
      "Testing Accuracy :  0.5988293247178014\n",
      "[[1108   38  108]\n",
      " [ 289  215  162]\n",
      " [ 407   86  329]]\n",
      "F1-Score :  0.6024799416484318\n",
      "Time Taken : 54.4398512840271\n",
      "(0.6346892677912359, 0.0035942320133563617, 0.5988293247178014, 0.012945255007082053, 0.6024799416484318, 54.4398512840271)\n"
     ]
    }
   ],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Random Forest Classifier\")\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(rf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "\n",
    "    # predicting test set results\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(rf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = random_forest(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "print(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup= random_forest(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)\n",
    "print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb897751-443b-46d9-b820-1fe02f0895ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.6077877692384112\n",
      "Testing Accuracy :  0.5762155478441227\n",
      "[[1165   23    0    2   43    0   21]\n",
      " [ 138  134    0    1   47    0   18]\n",
      " [  51    4    2    2    4    0   10]\n",
      " [ 139    8    0   17   18    0   18]\n",
      " [ 229   15    0    0  213    0   40]\n",
      " [  47    8    0    1    5    0   14]\n",
      " [ 138   21    0    2   49    0   95]]\n",
      "F1-Score :  0.5929978118161926\n",
      "Time Taken : 100.86223578453064\n",
      "(0.6077877692384112, 0.0050097755970676386, 0.5762155478441227, 0.011786676730148283, 0.5929978118161926, 100.86223578453064)\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.6668797836490599\n",
      "Testing Accuracy :  0.6269062529084068\n",
      "[[1100   43  111]\n",
      " [ 248  273  145]\n",
      " [ 322   78  422]]\n",
      "F1-Score :  0.6546316557257477\n",
      "Time Taken : 89.78336358070374\n",
      "(0.6668797836490599, 0.006719201969387679, 0.6269062529084068, 0.011485915978125406, 0.6546316557257477, 89.7843656539917)\n"
     ]
    }
   ],
   "source": [
    "def sv_classifier(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Support Vector Classifier\")\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(clf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    # predicting test set results\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(clf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "print(tup)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "print(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "861c972c-56cd-457d-9433-c8552537cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_senti = np.zeros((y_train_senti.shape[0], 3))\n",
    "for i in range(y_train_senti.shape[0]):\n",
    "    if y_train_senti[i] == 0:\n",
    "        Y_train_senti[i,:] = [1, 0, 0]\n",
    "    if y_train_senti[i] == 1:\n",
    "        Y_train_senti[i,:] = [0, 1, 0]\n",
    "    if y_train_senti[i] == 2:\n",
    "        Y_train_senti[i,:] = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1388dbaa-c801-4b7c-9717-ed36e5ac5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_senti = np.zeros((y_test_senti.shape[0], 3))\n",
    "for i in range(y_test_senti.shape[0]):\n",
    "    if y_test_senti[i] == 0:\n",
    "        Y_test_senti[i,:] = [1, 0, 0]\n",
    "    if y_test_senti[i] == 1:\n",
    "        Y_test_senti[i,:] = [0, 1, 0]\n",
    "    if y_test_senti[i] == 2:\n",
    "        Y_test_senti[i,:] = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ebab636-e856-4cb6-beca-a2db35f91e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2742, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_glove.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b27471a2-3f8b-4f48-b509-d784c928f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_x, train_y = X_train_glove.astype(np.float32), Y_train_senti.astype(np.int)\n",
    "valid_x, valid_y = X_test_glove.astype(np.float32), Y_test_senti.astype(np.int)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(300, 3000)\n",
    "        self.fc2 = nn.Linear(3000, 2000)\n",
    "        self.fc3 = nn.Linear(2000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 300)\n",
    "        self.fc6 = nn.Linear(300, 3)\n",
    "#         self.fc5 = nn.Linear(2000, 1000)\n",
    "#         self.fc6 = nn.Linear(1000, 500)\n",
    "#         self.fc7 = nn.Linear(500, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "\n",
    "givendata_train = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "givendata_test = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "trainset_loader = DataLoader(givendata_train, batch_size=64, shuffle=True, num_workers=1)\n",
    "validset_loader = DataLoader(givendata_test, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "def train(max_iters):\n",
    "    model.train()\n",
    "    Taccuracies = []\n",
    "    Tlosses = []\n",
    "    for itr in range(max_iters):\n",
    "        correct = 0\n",
    "        Tloss = 0\n",
    "        num = 0\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target = torch.max(target, 1)[1]\n",
    "            loss = nn.functional.cross_entropy(output, target)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            Tloss = Tloss + loss.item()\n",
    "\n",
    "            pred = torch.max(output, 1)[1] \n",
    "            correct += pred.eq(target).sum().item()\n",
    "            num = num + 1\n",
    "        \n",
    "        Taccuracies.append(100. * correct / (num*64))\n",
    "        Tlosses.append(Tloss/num)\n",
    "        if itr % 50 == 0:\n",
    "            print('Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "            print('Loss: {:.6f}'.format(Tloss/num))\n",
    "            \n",
    "    return Taccuracies, Tlosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3370019a-2876-4e95-8881-c1111001d0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            903,000\n",
      "├─Linear: 1-2                            6,002,000\n",
      "├─Linear: 1-3                            2,001,000\n",
      "├─Linear: 1-4                            500,500\n",
      "├─Linear: 1-5                            150,300\n",
      "├─Linear: 1-6                            903\n",
      "=================================================================\n",
      "Total params: 9,557,703\n",
      "Trainable params: 9,557,703\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            903,000\n",
      "├─Linear: 1-2                            6,002,000\n",
      "├─Linear: 1-3                            2,001,000\n",
      "├─Linear: 1-4                            500,500\n",
      "├─Linear: 1-5                            150,300\n",
      "├─Linear: 1-6                            903\n",
      "=================================================================\n",
      "Total params: 9,557,703\n",
      "Trainable params: 9,557,703\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_x, train_y = X_train_glove.astype(np.float32), Y_train_senti.astype(np.int)\n",
    "valid_x, valid_y = X_test_glove.astype(np.float32), Y_test_senti.astype(np.int)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(300, 3000)\n",
    "        self.fc2 = nn.Linear(3000, 2000)\n",
    "        self.fc3 = nn.Linear(2000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 300)\n",
    "        self.fc6 = nn.Linear(300, 3)\n",
    "#         self.fc5 = nn.Linear(2000, 1000)\n",
    "#         self.fc6 = nn.Linear(1000, 500)\n",
    "#         self.fc7 = nn.Linear(500, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "print(summary(model))\n",
    "\n",
    "givendata_train = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "givendata_test = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "trainset_loader = DataLoader(givendata_train, batch_size=64, shuffle=True, num_workers=1)\n",
    "validset_loader = DataLoader(givendata_test, batch_size=64, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15d23dcf-7049-48d7-8190-d1576eb6f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 42.81 %\n",
      "Loss: 1.088486\n",
      "Accuracy 47.00 %\n",
      "Loss: 1.005750\n",
      "Accuracy 59.87 %\n",
      "Loss: 0.849135\n",
      "Accuracy 61.63 %\n",
      "Loss: 0.822001\n",
      "Accuracy 66.51 %\n",
      "Loss: 0.757556\n",
      "Accuracy 74.02 %\n",
      "Loss: 0.615821\n"
     ]
    }
   ],
   "source": [
    "Taccuracies, Tlosses = train(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d72f27d-8697-41ab-b4e0-5ba7e5db9a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 62.03 %\n",
      "Loss: 1.054303\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "Taccuracies = []\n",
    "Tlosses = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    Tloss = 0\n",
    "    num = 0\n",
    "    for batch_idx, (data, target) in enumerate(validset_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        target = torch.max(target, 1)[1]\n",
    "        loss = nn.functional.cross_entropy(output, target)            \n",
    "        Tloss = Tloss + loss.item()\n",
    "\n",
    "        pred = torch.max(output, 1)[1] \n",
    "        correct += pred.eq(target).sum().item()\n",
    "        num = num + 1\n",
    "\n",
    "    Taccuracies.append(100. * correct / (num*64))\n",
    "    Tlosses.append(Tloss/num)\n",
    "    print('Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "    print('Loss: {:.6f}'.format(Tloss/num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ade6945c-a965-4ca1-8204-9442047de698",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_emo = np.zeros((y_train_emo.shape[0], 7))\n",
    "for i in range(y_train_emo.shape[0]):\n",
    "    if y_train_emo[i] == 0:\n",
    "        Y_train_emo[i,:] = [1, 0, 0, 0, 0, 0, 0]\n",
    "    if y_train_emo[i] == 1:\n",
    "        Y_train_emo[i,:] = [0, 1, 0, 0, 0, 0, 0]\n",
    "    if y_train_emo[i] == 2:\n",
    "        Y_train_emo[i,:] = [0, 0, 1, 0, 0, 0, 0]\n",
    "    if y_train_emo[i] == 3:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 1, 0, 0, 0]\n",
    "    if y_train_emo[i] == 4:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 0, 1, 0, 0]\n",
    "    if y_train_emo[i] == 5:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 0, 0, 1, 0]\n",
    "    if y_train_emo[i] == 6:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 0, 0, 0, 1]\n",
    "        \n",
    "        \n",
    "Y_test_emo = np.zeros((y_test_emo.shape[0], 7))\n",
    "for i in range(y_test_emo.shape[0]):\n",
    "    if y_test_emo[i] == 0:\n",
    "        Y_test_emo[i,:] = [1, 0, 0, 0, 0, 0, 0]\n",
    "    if y_test_emo[i] == 1:\n",
    "        Y_test_emo[i,:] = [0, 1, 0, 0, 0, 0, 0]\n",
    "    if y_test_emo[i] == 2:\n",
    "        Y_test_emo[i,:] = [0, 0, 1, 0, 0, 0, 0]\n",
    "    if y_test_emo[i] == 3:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 1, 0, 0, 0]\n",
    "    if y_test_emo[i] == 4:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 0, 1, 0, 0]\n",
    "    if y_test_emo[i] == 5:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 0, 0, 1, 0]\n",
    "    if y_test_emo[i] == 6:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3305c5ed-a3f2-466a-b489-c10be9dfb6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_x, train_y = X_train_glove.astype(np.float32), Y_train_emo.astype(np.int)\n",
    "valid_x, valid_y = X_test_glove.astype(np.float32), Y_test_emo.astype(np.int)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(300, 3000)\n",
    "        self.fc2 = nn.Linear(3000, 2000)\n",
    "        self.fc3 = nn.Linear(2000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 300)\n",
    "        self.fc6 = nn.Linear(300, 7)\n",
    "#         self.fc5 = nn.Linear(2000, 1000)\n",
    "#         self.fc6 = nn.Linear(1000, 500)\n",
    "#         self.fc7 = nn.Linear(500, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "\n",
    "givendata_train = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "givendata_test = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "trainset_loader = DataLoader(givendata_train, batch_size=64, shuffle=True, num_workers=1)\n",
    "validset_loader = DataLoader(givendata_test, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "def train(max_iters):\n",
    "    model.train()\n",
    "    Taccuracies = []\n",
    "    Tlosses = []\n",
    "    for itr in range(max_iters):\n",
    "        correct = 0\n",
    "        Tloss = 0\n",
    "        num = 0\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target = torch.max(target, 1)[1]\n",
    "            loss = nn.functional.cross_entropy(output, target)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            Tloss = Tloss + loss.item()\n",
    "\n",
    "            pred = torch.max(output, 1)[1] \n",
    "            correct += pred.eq(target).sum().item()\n",
    "            num = num + 1\n",
    "        \n",
    "        Taccuracies.append(100. * correct / (num*64))\n",
    "        Tlosses.append(Tloss/num)\n",
    "        if itr % 50 == 0:\n",
    "            print('Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "            print('Loss: {:.6f}'.format(Tloss/num))\n",
    "            \n",
    "    return Taccuracies, Tlosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa24548b-4233-411c-8ce3-bac02f20113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 46.02 %\n",
      "Loss: 1.864360\n",
      "Accuracy 52.52 %\n",
      "Loss: 1.395798\n",
      "Accuracy 54.28 %\n",
      "Loss: 1.322285\n",
      "Accuracy 54.38 %\n",
      "Loss: 1.290819\n",
      "Accuracy 55.13 %\n",
      "Loss: 1.239628\n",
      "Accuracy 59.96 %\n",
      "Loss: 1.098693\n"
     ]
    }
   ],
   "source": [
    "Taccuracies, Tlosses = train(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45b78937-7515-46a4-bbca-664c4257d833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 54.98 %\n",
      "Loss: 1.431657\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "Taccuracies = []\n",
    "Tlosses = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    Tloss = 0\n",
    "    num = 0\n",
    "    for batch_idx, (data, target) in enumerate(validset_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        target = torch.max(target, 1)[1]\n",
    "        loss = nn.functional.cross_entropy(output, target)            \n",
    "        Tloss = Tloss + loss.item()\n",
    "\n",
    "        pred = torch.max(output, 1)[1] \n",
    "        correct += pred.eq(target).sum().item()\n",
    "        num = num + 1\n",
    "\n",
    "    Taccuracies.append(100. * correct / (num*64))\n",
    "    Tlosses.append(Tloss/num)\n",
    "    print('Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "    print('Loss: {:.6f}'.format(Tloss/num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402178b3-6fe6-498e-9039-e1364a57506a",
   "metadata": {},
   "source": [
    "### Feature Engineering ---- Seems useless for GLOVE vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9ffb4eb-820c-4b53-b81a-021656a65ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_remover(x):\n",
    "    X_t1 = x\n",
    "    for i in range(len(x)):\n",
    "        X_t1[i] = [char for char in x[i] if char not in string.punctuation]\n",
    "        X_t1[i]=''.join(X_t1[i])\n",
    "    return X_t1\n",
    "\n",
    "#All useless punctuations are removed\n",
    "X_tr1 = punctuation_remover(X_train_copy1)\n",
    "X_te1 = punctuation_remover(X_test_copy1)\n",
    "\n",
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in X_tr1])\n",
    "X_train_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_train_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n",
    "doc_glove_vectors2 = np.array([nlp(str(doc)).vector for doc in X_te1])\n",
    "X_test_glove = np.zeros((doc_glove_vectors2.shape[0], 300))\n",
    "for i in range(doc_glove_vectors2.shape[0]):\n",
    "    if (doc_glove_vectors2[i].shape[0] == 300):\n",
    "        X_test_glove[i,:] = doc_glove_vectors2[i][:]\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e44efc9b-b31c-45a0-93bd-cbc24f617577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5116734234767872\n",
      "Testing Accuracy :  0.4894207118450268\n",
      "[[1116   37    1   19   50    3   23]\n",
      " [ 196  110    0    9   14    0   15]\n",
      " [  54    3    2    3    1    0    4]\n",
      " [ 153    8    0   17    5    1   10]\n",
      " [ 366   13    1    6   92    1   12]\n",
      " [  58    6    0    2    0    6    7]\n",
      " [ 258   13    1    4   10    2   30]]\n",
      "Time Taken : 16.00754189491272\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5519780556431939\n",
      "Testing Accuracy :  0.5321001688537885\n",
      "[[975  96 178]\n",
      " [362 180 125]\n",
      " [433  44 349]]\n",
      "Time Taken : 9.973003625869751\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter = 1e4)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5556d38d-b823-4805-8990-db21111b3f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Random Forest Classifier\n",
      "Training Accuracy :  0.5100308805551433\n",
      "Testing Accuracy :  0.48067488333133895\n",
      "[[1208   12    2    2   20    0    5]\n",
      " [ 250   77    1    1    9    1    5]\n",
      " [  60    4    1    0    0    0    2]\n",
      " [ 180    5    1    6    1    1    0]\n",
      " [ 411    7    1    2   69    0    1]\n",
      " [  71    2    1    1    1    2    1]\n",
      " [ 295    5    0    1    3    0   14]]\n",
      "Time Taken : 151.55781745910645\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Random Forest Classifier\n",
      "Training Accuracy :  0.5430423145493596\n",
      "Testing Accuracy :  0.5120364830547911\n",
      "[[1092   37  120]\n",
      " [ 452  136   79]\n",
      " [ 544   22  260]]\n",
      "Time Taken : 130.1093897819519\n"
     ]
    }
   ],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Random Forest Classifier\")\n",
    "    rf = RandomForestClassifier(random_state=42, n_estimators  = 180)\n",
    "    rf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(rf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "\n",
    "    # predicting test set results\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(rf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = random_forest(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup= random_forest(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9467fec1-9db5-4919-81dd-ebbf3cf7bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.5186937304866192\n",
      "Testing Accuracy :  0.49599005491072023\n",
      "[[1218    9    0    3   17    0    2]\n",
      " [ 238   94    0    0    9    0    3]\n",
      " [  62    3    0    1    0    0    1]\n",
      " [ 171    8    0   13    1    1    0]\n",
      " [ 396    7    1    2   84    0    1]\n",
      " [  71    4    0    1    0    1    2]\n",
      " [ 294    6    0    1    6    0   11]]\n",
      "Time Taken : 128.2013020515442\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.561918027900991\n",
      "Testing Accuracy :  0.5466913964341271\n",
      "[[1068   50  131]\n",
      " [ 389  173  105]\n",
      " [ 460   27  339]]\n",
      "Time Taken : 115.29567837715149\n"
     ]
    }
   ],
   "source": [
    "def sv_classifier(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Support Vector Classifier\")\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(clf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    # predicting test set results\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(clf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b516267-7d3e-4db8-8b3e-d5fce82f2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = set(stopwords.words('english'))\n",
    "#print(StopWords)\n",
    "\n",
    "#train2 and test2 after stopwords removed \n",
    "def stopwords_remover(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = [word for word in x[i].split() if word.lower() not in StopWords]\n",
    "        \n",
    "    a = \"\"\n",
    "    for i in range(len(x)):\n",
    "        for word in x[i]:\n",
    "            #print(word)\n",
    "            a = a +word +\" \"\n",
    "        x[i]=a\n",
    "        a=\"\"\n",
    "    return x\n",
    "\n",
    "X_tr1 = stopwords_remover(X_train_copy1)\n",
    "X_te1 = stopwords_remover(X_test_copy1)\n",
    "\n",
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in X_tr1])\n",
    "X_train_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_train_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n",
    "doc_glove_vectors2 = np.array([nlp(str(doc)).vector for doc in X_te1])\n",
    "X_test_glove = np.zeros((doc_glove_vectors2.shape[0], 300))\n",
    "for i in range(doc_glove_vectors2.shape[0]):\n",
    "    if (doc_glove_vectors2[i].shape[0] == 300):\n",
    "        X_test_glove[i,:] = doc_glove_vectors2[i][:]\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60c97e2c-3182-4429-bb7a-22826e22bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5041042165737424\n",
      "Testing Accuracy :  0.45697419329105343\n",
      "[[1115   36    2   19   44    4   29]\n",
      " [ 213   81    1    8   18    1   22]\n",
      " [  56    3    0    3    0    0    5]\n",
      " [ 151    6    0   17    6    3   11]\n",
      " [ 365   15    1    7   96    1    6]\n",
      " [  55    6    1    3    0    7    7]\n",
      " [ 242   11    3    6   18    5   33]]\n",
      "Time Taken : 14.356157779693604\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.543223382927613\n",
      "Testing Accuracy :  0.5233523460040154\n",
      "[[1003   87  159]\n",
      " [ 386  183   98]\n",
      " [ 448   60  318]]\n",
      "Time Taken : 8.695868253707886\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter = 1e4)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4620ad8c-fc55-42f6-ae29-64e0d1a6e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def stem_sentences(x):\n",
    "    sen_list = []\n",
    "    for sentence in x:\n",
    "        tokens = sentence.split()\n",
    "#         print(tokens)\n",
    "        stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "#         print(stemmed_tokens)\n",
    "        stemmed_string =  ' '.join(stemmed_tokens)\n",
    "        sen_list.append(stemmed_string)\n",
    "    sen_list_arr = np.asarray(sen_list)\n",
    "    return sen_list_arr\n",
    "\n",
    "X_tr1 = stem_sentences(X_train_copy1)\n",
    "X_te1 = stem_sentences(X_test_copy1)\n",
    "\n",
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in X_tr1])\n",
    "X_train_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_train_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n",
    "doc_glove_vectors2 = np.array([nlp(str(doc)).vector for doc in X_te1])\n",
    "X_test_glove = np.zeros((doc_glove_vectors2.shape[0], 300))\n",
    "for i in range(doc_glove_vectors2.shape[0]):\n",
    "    if (doc_glove_vectors2[i].shape[0] == 300):\n",
    "        X_test_glove[i,:] = doc_glove_vectors2[i][:]\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da336fa8-9e8d-4fb4-ab33-9caedc6bdbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5017330771107706\n",
      "Testing Accuracy :  0.45369085131559705\n",
      "[[1112   28    3   22   54    4   26]\n",
      " [ 227   80    1    7   14    1   14]\n",
      " [  55    4    0    3    0    0    5]\n",
      " [ 157    7    1   16    6    5    2]\n",
      " [ 373   15    1    8   83    2    9]\n",
      " [  57    7    0    2    1    7    5]\n",
      " [ 261    8    2    5   15    2   25]]\n",
      "Time Taken : 13.163366556167603\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5399411652473416\n",
      "Testing Accuracy :  0.4817790807440203\n",
      "[[998  91 160]\n",
      " [400 176  91]\n",
      " [487  54 285]]\n",
      "Time Taken : 7.963275194168091\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter = 1e4)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "803647d2-0948-4743-8654-e5c53ff0eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr1 = stem_sentences(stopwords_remover(punctuation_remover(X_train_copy1)))\n",
    "X_te1 = stem_sentences(stopwords_remover(punctuation_remover(X_test_copy1)))\n",
    "\n",
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in X_tr1])\n",
    "X_train_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_train_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n",
    "doc_glove_vectors2 = np.array([nlp(str(doc)).vector for doc in X_te1])\n",
    "X_test_glove = np.zeros((doc_glove_vectors2.shape[0], 300))\n",
    "for i in range(doc_glove_vectors2.shape[0]):\n",
    "    if (doc_glove_vectors2[i].shape[0] == 300):\n",
    "        X_test_glove[i,:] = doc_glove_vectors2[i][:]\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1b0690f-acc7-42b7-bfcb-157f3ef22b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5017330771107706\n",
      "Testing Accuracy :  0.45369085131559705\n",
      "[[1112   28    3   22   54    4   26]\n",
      " [ 227   80    1    7   14    1   14]\n",
      " [  55    4    0    3    0    0    5]\n",
      " [ 157    7    1   16    6    5    2]\n",
      " [ 373   15    1    8   83    2    9]\n",
      " [  57    7    0    2    1    7    5]\n",
      " [ 261    8    2    5   15    2   25]]\n",
      "Time Taken : 13.201356887817383\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Multinomial Logistic Regression\n",
      "Training Accuracy :  0.5399411652473416\n",
      "Testing Accuracy :  0.4817790807440203\n",
      "[[998  91 160]\n",
      " [400 176  91]\n",
      " [487  54 285]]\n",
      "Time Taken : 7.921770334243774\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', max_iter = 1e4)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2e728a6-3908-4a35-ac0e-90d378ff7f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.5169609859164883\n",
      "Testing Accuracy :  0.48067355377394866\n",
      "[[1206    7    1    7   22    1    5]\n",
      " [ 268   58    0    4    8    0    6]\n",
      " [  62    3    0    2    0    0    0]\n",
      " [ 168    6    0   14    3    3    0]\n",
      " [ 406    8    0    5   71    1    0]\n",
      " [  69    3    0    0    1    2    4]\n",
      " [ 291    7    0    1    8    1   10]]\n",
      "Time Taken : 133.3147828578949\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.550245976154342\n",
      "Testing Accuracy :  0.5215222102562057\n",
      "[[1077   41  131]\n",
      " [ 426  152   89]\n",
      " [ 520   31  275]]\n",
      "Time Taken : 104.24002289772034\n"
     ]
    }
   ],
   "source": [
    "def sv_classifier(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Support Vector Classifier\")\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(clf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    # predicting test set results\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(clf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_emo, X_test_glove, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = sv_classifier(X_train_glove, y_train_senti, X_test_glove, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85cd6e-3f12-4ec4-949a-d42cc36e4f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
