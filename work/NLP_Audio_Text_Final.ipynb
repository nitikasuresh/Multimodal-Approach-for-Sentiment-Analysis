{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ae57bd-b6a6-437e-87f6-b8dd19162105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95529737-d4f8-4d07-b63b-b4fe34773bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_audio = pd.read_csv('audio_features.csv')\n",
    "#data_audio = data_audio.loc[data_audio['2'] == 'Monica']\n",
    "data_audio  = np.asarray(data_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06203e2a-6254-4367-a54f-7e54f3c2f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data1.csv')\n",
    "df2 = pd.read_csv('data2.csv')\n",
    "df3 = pd.read_csv('data3.csv')\n",
    "df4 = pd.read_csv('data4.csv')\n",
    "\n",
    "frames = [df1,df2,df3,df4]\n",
    "data_text = pd.concat(frames) #complete dataset\n",
    "#data_text = data_text.loc[data_text['2'] == 'Monica']\n",
    "mat =np.asarray([]) #emo\n",
    "mat1 =np.asarray([]) #senti\n",
    "data_text = np.asarray(data_text[['1','3','4']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03c3fd20-6d16-4256-92cb-372a6faad066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [2, 2],\n",
       "       [0, 0],\n",
       "       ...,\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0]], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_audio[:,4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fff4b39-ec01-4f89-a927-a4695dd60604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [2, 2],\n",
       "       [0, 0],\n",
       "       ...,\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0]], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4da36df4-4b66-43dc-8fd4-57a5be24d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(data_text[:,1:3], data_audio[:,4:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4773e6c8-6968-4025-b843-449d708c14d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word vectors: 684830\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "print('Total word vectors:', total_vectors)\n",
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in data_text[:,0]])\n",
    "X_glove = np.zeros((doc_glove_vectors.shape[0], 300))\n",
    "for i in range(doc_glove_vectors.shape[0]):\n",
    "    if (doc_glove_vectors[i].shape[0] == 300):\n",
    "        X_glove[i,:] = doc_glove_vectors[i][:]\n",
    "    else:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41ddb840-08af-4ec2-95bd-efb559fc5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "feat_combined = np.concatenate((scaler.fit_transform(X_glove), scaler.fit_transform(data_audio[:,13:])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "959ba815-cd83-4c56-9298-63ba6f8f9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feat_combined, data_audio[:,2:6], test_size=0.2, shuffle = True)\n",
    "\n",
    "y_train = y_train[y_train[:,1] == \"Rachel\"]\n",
    "\n",
    "# emotion (7)\n",
    "y_train_emo = y_train[:,2] *1.0\n",
    "y_train_emo = y_train_emo.astype('float')\n",
    "y_test_emo = y_test[:,2] *1.0\n",
    "y_test_emo = y_test_emo.astype('float')\n",
    "\n",
    "# sentiment(3)\n",
    "y_train_senti = y_train[:,3] *1.0\n",
    "y_train_senti = y_train_senti.astype('float')\n",
    "y_test_senti = y_test[:,3] *1.0\n",
    "y_test_senti = y_test_senti.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ae37c-ab5e-4488-a2f5-f8356752298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Multinomial Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    print(\"Multinomial Logistic Regression\")\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(penalty='l1',multi_class='multinomial', max_iter = 1e4, solver = 'saga')\n",
    "    #penalty to handle size better\n",
    "    \n",
    "    #https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451\n",
    "    \n",
    "    logreg.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(logreg, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    accuracy_test_data = cross_val_score(logreg, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.mean(accuracy_test_data), time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = logistic_regression(X_train, y_train_emo, X_test, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = logistic_regression(X_train, y_train_senti, X_test, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbabff7b-7302-4a08-a793-7d8d642b840c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Random Forest Classifier\n",
      "Training Accuracy :  0.5538027061325898\n",
      "Testing Accuracy :  0.5269853615731324\n",
      "[[1240   17    0    0   24    0    3]\n",
      " [ 189   79    0    0   43    0   17]\n",
      " [  56    3    0    0    7    0    6]\n",
      " [ 169    4    0    0   13    0    7]\n",
      " [ 301    9    0    0  150    0   15]\n",
      " [  54    4    0    0    5    0    8]\n",
      " [ 197    9    0    0   59    0   54]]\n",
      "F1-Score :  0.5554339897884756\n",
      "Time Taken : 162.17528986930847\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Random Forest Classifier\n",
      "Training Accuracy :  0.6116174735141773\n",
      "Testing Accuracy :  0.5878990334117772\n",
      "[[1180   18   86]\n",
      " [ 299  181  155]\n",
      " [ 429   79  315]]\n",
      "F1-Score :  0.611232676878191\n",
      "Time Taken : 139.73934364318848\n"
     ]
    }
   ],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Random Forest Classifier\")\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(rf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "\n",
    "    # predicting test set results\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(rf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = random_forest(X_train, y_train_emo, X_test, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup= random_forest(X_train, y_train_senti, X_test, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9650b-a77f-4f4f-abad-2ed97149306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sv_classifier(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Support Vector Classifier\")\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(clf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    # predicting test set results\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(clf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = sv_classifier(X_train, y_train_emo, X_test, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = sv_classifier(X_train, y_train_senti, X_test, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b451dd29-8859-4915-85bb-729f65e58c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "774991a1-7086-45ad-a7cb-aa0bb4f88cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8449567202155046\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=300, svd_solver='full')\n",
    "X_pca = pca.fit_transform(data_audio[:,13:].copy())\n",
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac571c8a-4883-457c-96b4-0d5e64c96878",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_combined = np.concatenate((scaler.fit_transform(X_glove), scaler.fit_transform(X_pca)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa9e9114-94d3-4a6c-b05a-4bc03b07e222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13708, 600)\n"
     ]
    }
   ],
   "source": [
    "print(feat_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "071c2c1c-09b2-4328-b5d4-22a609cf095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feat_combined, data_audio[:,2:6], test_size=0.2, shuffle = True)\n",
    "# emotion (7)\n",
    "# X_test = X_test[y_test[:,1] == \"Joey\"]\n",
    "# y_test = y_test[y_test[:,1] == \"Joey\"]\n",
    "# #args = y_test[:,1] == \"Chandler\"\n",
    "\n",
    "# y_train_emo = y_train[:,2] *1.0\n",
    "# y_train_emo = y_train_emo.astype('float')\n",
    "# y_test_emo = y_test[:,2] *1.0\n",
    "# y_test_emo = y_test_emo.astype('float')\n",
    "\n",
    "# # sentiment(3)\n",
    "# y_train_senti = y_train[:,3] *1.0\n",
    "# y_train_senti = y_train_senti.astype('float')\n",
    "# y_test_senti = y_test[:,3] *1.0\n",
    "# y_test_senti = y_test_senti.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96509211-6828-47d6-85d1-d7c9e227a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testi = X_test.copy()\n",
    "y_testi = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cbf3f8f-3dc5-45ae-92b1-5fe5e49af7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = y_train[y_train[:,1] == \"Chandler\"]\n",
    "# X_test = X_testi[y_testi[:,1] == \"Chandler\"]\n",
    "# y_test = y_testi[y_testi[:,1] == \"Chandler\"]\n",
    "# #args = y_test[:,1] == \"Chandler\"\n",
    "# print(X_test.shape, y_test.shape)\n",
    "\n",
    "y_train_emo = y_train[:,2] *1.0\n",
    "y_train_emo = y_train_emo.astype('float')\n",
    "y_test_emo = y_test[:,2] *1.0\n",
    "y_test_emo = y_test_emo.astype('float')\n",
    "\n",
    "# sentiment(3)\n",
    "y_train_senti = y_train[:,3] *1.0\n",
    "y_train_senti = y_train_senti.astype('float')\n",
    "y_test_senti = y_test[:,3] *1.0\n",
    "y_test_senti = y_test_senti.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f79a33fb-2732-4e7f-b590-6495e4b122f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.598213051305617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 199, in fit\n",
      "    y = self._validate_targets(y)\n",
      "  File \"C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 722, in _validate_targets\n",
      "    % len(cls)\n",
      "ValueError: The number of classes has to be greater than one; got 1 class\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy :  nan\n",
      "F1-Score :  0.9291217257318953\n",
      "[[1206   22    1    3   49   17]\n",
      " [   0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0]]\n",
      "Time Taken : 196.60517024993896\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.6556633541462206\n",
      "Testing Accuracy :  nan\n",
      "F1-Score :  0.884437596302003\n",
      "[[1148   48  102]\n",
      " [   0    0    0]\n",
      " [   0    0    0]]\n",
      "Time Taken : 170.23407292366028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 199, in fit\n",
      "    y = self._validate_targets(y)\n",
      "  File \"C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 722, in _validate_targets\n",
      "    % len(cls)\n",
      "ValueError: The number of classes has to be greater than one; got 1 class\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "def sv_classifier(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Support Vector Classifier\")\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(clf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    # predicting test set results\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(clf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = sv_classifier(X_train, y_train_emo, X_test, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = sv_classifier(X_train, y_train_senti, X_test, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc9156-3b47-466d-8553-465f57e65c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72661e78-f7c0-4699-979d-7a8ef14f26d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EMOTIONS (7): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.597575570899535\n",
      "Testing Accuracy :  0.5692938720699879\n",
      "F1-Score :  0.6032093362509118\n",
      "[[1228   19    0    2   51    0   12]\n",
      " [ 136  111    0    0   63    0   24]\n",
      " [  42    3    0    2    6    0   13]\n",
      " [ 170   10    0    9   11    0   12]\n",
      " [ 171   16    1    1  207    0   39]\n",
      " [  45    6    0    1    5    0   11]\n",
      " [ 140   21    0    1   54    0   99]]\n",
      "Time Taken : 250.80271935462952\n",
      "\n",
      "FOR SENTIMENTS (3): \n",
      "Support Vector Classifier\n",
      "Training Accuracy :  0.6546614923343148\n",
      "Testing Accuracy :  0.6404059138712722\n",
      "F1-Score :  0.675054704595186\n",
      "[[1162   52   98]\n",
      " [ 192  281  133]\n",
      " [ 345   71  408]]\n",
      "Time Taken : 228.58726835250854\n"
     ]
    }
   ],
   "source": [
    "def sv_classifier(X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    print(\"Support Vector Classifier\")\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy_train_data = cross_val_score(clf, X_train,y_train,cv=5)\n",
    "    print(\"Training Accuracy : \",np.mean(accuracy_train_data))\n",
    "    \n",
    "    # predicting test set results\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_test_data = cross_val_score(clf, X_test,y_test,cv=5)\n",
    "    print(\"Testing Accuracy : \",np.mean(accuracy_test_data))\n",
    "    # making the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1-Score : \",f1)\n",
    "    print(cm)\n",
    "    print(\"Time Taken :\", (time.time()-start))\n",
    "    return np.mean(accuracy_train_data), np.std(accuracy_train_data), np.mean(accuracy_test_data), np.std(accuracy_test_data), f1, time.time()-start\n",
    "\n",
    "print(\"FOR EMOTIONS (7): \")\n",
    "tup = sv_classifier(X_train, y_train_emo, X_test, y_test_emo)\n",
    "d = np.asarray(tup)\n",
    "mat = np.append(mat,d)\n",
    "print(\"\\nFOR SENTIMENTS (3): \")\n",
    "tup = sv_classifier(X_train, y_train_senti, X_test, y_test_senti)\n",
    "d = np.asarray(tup)\n",
    "mat1 = np.append(mat1,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5120843-08a8-4cd5-968b-8900c9ade6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_senti = np.zeros((y_train_senti.shape[0], 3))\n",
    "for i in range(y_train_senti.shape[0]):\n",
    "    if y_train_senti[i] == 0:\n",
    "        Y_train_senti[i,:] = [1, 0, 0]\n",
    "    if y_train_senti[i] == 1:\n",
    "        Y_train_senti[i,:] = [0, 1, 0]\n",
    "    if y_train_senti[i] == 2:\n",
    "        Y_train_senti[i,:] = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0974fd13-4761-4348-b16e-eedb38824670",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_senti = np.zeros((y_test_senti.shape[0], 3))\n",
    "for i in range(y_test_senti.shape[0]):\n",
    "    if y_test_senti[i] == 0:\n",
    "        Y_test_senti[i,:] = [1, 0, 0]\n",
    "    if y_test_senti[i] == 1:\n",
    "        Y_test_senti[i,:] = [0, 1, 0]\n",
    "    if y_test_senti[i] == 2:\n",
    "        Y_test_senti[i,:] = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f494a6c-5f35-4007-87d5-b2c7a8055254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_x, train_y = X_train.astype(np.float32), Y_train_senti.astype(np.int)\n",
    "valid_x, valid_y = X_test.astype(np.float32), Y_test_senti.astype(np.int)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(600, 3000)\n",
    "        self.fc2 = nn.Linear(3000, 2000)\n",
    "        self.fc3 = nn.Linear(2000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 300)\n",
    "        self.fc6 = nn.Linear(300, 3)\n",
    "#         self.fc5 = nn.Linear(2000, 1000)\n",
    "#         self.fc6 = nn.Linear(1000, 500)\n",
    "#         self.fc7 = nn.Linear(500, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "\n",
    "givendata_train = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "givendata_test = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "trainset_loader = DataLoader(givendata_train, batch_size=64, shuffle=True, num_workers=1)\n",
    "validset_loader = DataLoader(givendata_test, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "def train(max_iters):\n",
    "    model.train()\n",
    "    Taccuracies = []\n",
    "    Tlosses = []\n",
    "    for itr in range(max_iters):\n",
    "        correct = 0\n",
    "        Tloss = 0\n",
    "        num = 0\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target = torch.max(target, 1)[1]\n",
    "            loss = nn.functional.cross_entropy(output, target)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            Tloss = Tloss + loss.item()\n",
    "\n",
    "            pred = torch.max(output, 1)[1] \n",
    "            correct += pred.eq(target).sum().item()\n",
    "            num = num + 1\n",
    "        \n",
    "        Taccuracies.append(100. * correct / (num*64))\n",
    "        Tlosses.append(Tloss/num)\n",
    "        if itr % 10 == 0:\n",
    "            print('Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "            print('Loss: {:.6f}'.format(Tloss/num))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            Tloss = 0\n",
    "            num = 0\n",
    "            for batch_idx, (data, target) in enumerate(validset_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                target = torch.max(target, 1)[1]\n",
    "                loss = nn.functional.cross_entropy(output, target)            \n",
    "                Tloss = Tloss + loss.item()\n",
    "\n",
    "                pred = torch.max(output, 1)[1] \n",
    "                correct += pred.eq(target).sum().item()\n",
    "                num = num + 1\n",
    "\n",
    "            Taccuracies.append(100. * correct / (num*64))\n",
    "            Tlosses.append(Tloss/num)\n",
    "            if itr % 10 == 0:\n",
    "                print('Test Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "                print('Test Loss: {:.6f}'.format(Tloss/num))\n",
    "\n",
    "    return Taccuracies, Tlosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5fb3421-509a-4768-b83b-0680d1805e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 56.50 %\n",
      "Loss: 0.907798\n",
      "Test Accuracy 57.09 %\n",
      "Test Loss: 0.906664\n",
      "Accuracy 57.85 %\n",
      "Loss: 0.884336\n",
      "Test Accuracy 58.21 %\n",
      "Test Loss: 0.900535\n",
      "Accuracy 58.36 %\n",
      "Loss: 0.874906\n",
      "Test Accuracy 58.18 %\n",
      "Test Loss: 0.886666\n",
      "Accuracy 58.74 %\n",
      "Loss: 0.868387\n",
      "Test Accuracy 59.12 %\n",
      "Test Loss: 0.879699\n",
      "Accuracy 59.52 %\n",
      "Loss: 0.865622\n",
      "Test Accuracy 59.67 %\n",
      "Test Loss: 0.882686\n",
      "Accuracy 59.27 %\n",
      "Loss: 0.854918\n",
      "Test Accuracy 59.81 %\n",
      "Test Loss: 0.870758\n",
      "Accuracy 59.97 %\n",
      "Loss: 0.847769\n",
      "Test Accuracy 60.17 %\n",
      "Test Loss: 0.865348\n",
      "Accuracy 60.14 %\n",
      "Loss: 0.838698\n",
      "Test Accuracy 58.39 %\n",
      "Test Loss: 0.875197\n",
      "Accuracy 60.14 %\n",
      "Loss: 0.842262\n",
      "Test Accuracy 59.48 %\n",
      "Test Loss: 0.869486\n",
      "Accuracy 60.43 %\n",
      "Loss: 0.834213\n",
      "Test Accuracy 59.08 %\n",
      "Test Loss: 0.872016\n"
     ]
    }
   ],
   "source": [
    "Taccuracies, Tlosses = train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f842385-64bc-4bfc-ae68-97b3c45198e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 60.94 %\n",
      "Loss: 0.821893\n",
      "Test Accuracy 60.03 %\n",
      "Test Loss: 0.869604\n",
      "Accuracy 61.56 %\n",
      "Loss: 0.809883\n",
      "Test Accuracy 59.01 %\n",
      "Test Loss: 0.905895\n",
      "Accuracy 62.92 %\n",
      "Loss: 0.789188\n",
      "Test Accuracy 59.63 %\n",
      "Test Loss: 0.877293\n",
      "Accuracy 63.00 %\n",
      "Loss: 0.783700\n",
      "Test Accuracy 57.09 %\n",
      "Test Loss: 0.893886\n",
      "Accuracy 63.69 %\n",
      "Loss: 0.758338\n",
      "Test Accuracy 56.58 %\n",
      "Test Loss: 0.904735\n",
      "Accuracy 65.72 %\n",
      "Loss: 0.731071\n",
      "Test Accuracy 57.70 %\n",
      "Test Loss: 0.907084\n",
      "Accuracy 65.27 %\n",
      "Loss: 0.725349\n",
      "Test Accuracy 59.27 %\n",
      "Test Loss: 0.951403\n",
      "Accuracy 67.10 %\n",
      "Loss: 0.678583\n",
      "Test Accuracy 58.83 %\n",
      "Test Loss: 0.952845\n",
      "Accuracy 67.51 %\n",
      "Loss: 0.683906\n",
      "Test Accuracy 56.40 %\n",
      "Test Loss: 0.924327\n",
      "Accuracy 67.55 %\n",
      "Loss: 0.665514\n",
      "Test Accuracy 58.21 %\n",
      "Test Loss: 1.032440\n"
     ]
    }
   ],
   "source": [
    "Taccuracies, Tlosses = train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "839596e5-94ca-45d0-a741-206b1065e39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 70.03 %\n",
      "Loss: 0.608035\n",
      "Test Accuracy 54.03 %\n",
      "Test Loss: 1.031826\n",
      "Accuracy 67.43 %\n",
      "Loss: 0.676776\n",
      "Test Accuracy 53.20 %\n",
      "Test Loss: 0.986071\n",
      "Accuracy 71.17 %\n",
      "Loss: 0.602814\n",
      "Test Accuracy 50.80 %\n",
      "Test Loss: 1.149753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1816/328021055.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTaccuracies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1816/1720744158.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(max_iters)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sidenv\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sidenv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Taccuracies, Tlosses = train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3eff19ef-48b4-4493-a1e0-874d902c272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_emo = np.zeros((y_train_emo.shape[0], 7))\n",
    "for i in range(y_train_emo.shape[0]):\n",
    "    if y_train_emo[i] == 0:\n",
    "        Y_train_emo[i,:] = [1, 0, 0, 0, 0, 0, 0]\n",
    "    if y_train_emo[i] == 1:\n",
    "        Y_train_emo[i,:] = [0, 1, 0, 0, 0, 0, 0]\n",
    "    if y_train_emo[i] == 2:\n",
    "        Y_train_emo[i,:] = [0, 0, 1, 0, 0, 0, 0]\n",
    "    if y_train_emo[i] == 3:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 1, 0, 0, 0]\n",
    "    if y_train_emo[i] == 4:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 0, 1, 0, 0]\n",
    "    if y_train_emo[i] == 5:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 0, 0, 1, 0]\n",
    "    if y_train_emo[i] == 6:\n",
    "        Y_train_emo[i,:] = [0, 0, 0, 0, 0, 0, 1]\n",
    "        \n",
    "        \n",
    "Y_test_emo = np.zeros((y_test_emo.shape[0], 7))\n",
    "for i in range(y_test_emo.shape[0]):\n",
    "    if y_test_emo[i] == 0:\n",
    "        Y_test_emo[i,:] = [1, 0, 0, 0, 0, 0, 0]\n",
    "    if y_test_emo[i] == 1:\n",
    "        Y_test_emo[i,:] = [0, 1, 0, 0, 0, 0, 0]\n",
    "    if y_test_emo[i] == 2:\n",
    "        Y_test_emo[i,:] = [0, 0, 1, 0, 0, 0, 0]\n",
    "    if y_test_emo[i] == 3:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 1, 0, 0, 0]\n",
    "    if y_test_emo[i] == 4:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 0, 1, 0, 0]\n",
    "    if y_test_emo[i] == 5:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 0, 0, 1, 0]\n",
    "    if y_test_emo[i] == 6:\n",
    "        Y_test_emo[i,:] = [0, 0, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8ea9fc7-b46e-4312-a911-d9406d918e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\saisi\\anaconda3\\envs\\sidenv\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_x, train_y = X_train.astype(np.float32), Y_train_emo.astype(np.int)\n",
    "valid_x, valid_y = X_test.astype(np.float32), Y_test_emo.astype(np.int)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(600, 3000)\n",
    "        self.fc2 = nn.Linear(3000, 2000)\n",
    "        self.fc3 = nn.Linear(2000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 300)\n",
    "        self.fc6 = nn.Linear(300, 7)\n",
    "#         self.fc5 = nn.Linear(2000, 1000)\n",
    "#         self.fc6 = nn.Linear(1000, 500)\n",
    "#         self.fc7 = nn.Linear(500, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "\n",
    "givendata_train = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "givendata_test = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "trainset_loader = DataLoader(givendata_train, batch_size=64, shuffle=True, num_workers=1)\n",
    "validset_loader = DataLoader(givendata_test, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "def train(max_iters):\n",
    "    model.train()\n",
    "    Taccuracies = []\n",
    "    Tlosses = []\n",
    "    for itr in range(max_iters):\n",
    "        correct = 0\n",
    "        Tloss = 0\n",
    "        num = 0\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target = torch.max(target, 1)[1]\n",
    "            loss = nn.functional.cross_entropy(output, target)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            Tloss = Tloss + loss.item()\n",
    "\n",
    "            pred = torch.max(output, 1)[1] \n",
    "            correct += pred.eq(target).sum().item()\n",
    "            num = num + 1\n",
    "        \n",
    "        Taccuracies.append(100. * correct / (num*64))\n",
    "        Tlosses.append(Tloss/num)\n",
    "        if itr % 10 == 0:\n",
    "            print('Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "            print('Loss: {:.6f}'.format(Tloss/num))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            Tloss = 0\n",
    "            num = 0\n",
    "            for batch_idx, (data, target) in enumerate(validset_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                target = torch.max(target, 1)[1]\n",
    "                loss = nn.functional.cross_entropy(output, target)            \n",
    "                Tloss = Tloss + loss.item()\n",
    "\n",
    "                pred = torch.max(output, 1)[1] \n",
    "                correct += pred.eq(target).sum().item()\n",
    "                num = num + 1\n",
    "\n",
    "            Taccuracies.append(100. * correct / (num*64))\n",
    "            Tlosses.append(Tloss/num)\n",
    "            if itr % 10 == 0:\n",
    "                print('Test Accuracy {:.2f} %'.format(100. * correct / (num*64)))\n",
    "                print('Test Loss: {:.6f}'.format(Tloss/num))\n",
    "\n",
    "    return Taccuracies, Tlosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d952c4b1-21cd-429c-8098-f93be8a76388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 39.05 %\n",
      "Loss: 1.887980\n",
      "Test Accuracy 46.33 %\n",
      "Test Loss: 1.815119\n",
      "Accuracy 46.88 %\n",
      "Loss: 1.549549\n",
      "Test Accuracy 46.33 %\n",
      "Test Loss: 1.548742\n",
      "Accuracy 46.88 %\n",
      "Loss: 1.545137\n",
      "Test Accuracy 46.33 %\n",
      "Test Loss: 1.544692\n",
      "Accuracy 46.88 %\n",
      "Loss: 1.544661\n",
      "Test Accuracy 46.33 %\n",
      "Test Loss: 1.545851\n",
      "Accuracy 46.88 %\n",
      "Loss: 1.540194\n",
      "Test Accuracy 46.33 %\n",
      "Test Loss: 1.538619\n",
      "Accuracy 46.88 %\n",
      "Loss: 1.531877\n",
      "Test Accuracy 46.33 %\n",
      "Test Loss: 1.530019\n",
      "Accuracy 46.88 %\n",
      "Loss: 1.487429\n",
      "Test Accuracy 46.33 %\n",
      "Test Loss: 1.487566\n",
      "Accuracy 47.14 %\n",
      "Loss: 1.422493\n",
      "Test Accuracy 47.09 %\n",
      "Test Loss: 1.428863\n",
      "Accuracy 52.55 %\n",
      "Loss: 1.378669\n",
      "Test Accuracy 51.82 %\n",
      "Test Loss: 1.421778\n",
      "Accuracy 53.01 %\n",
      "Loss: 1.363971\n",
      "Test Accuracy 52.87 %\n",
      "Test Loss: 1.396018\n",
      "Accuracy 53.05 %\n",
      "Loss: 1.354155\n",
      "Test Accuracy 53.05 %\n",
      "Test Loss: 1.392738\n",
      "Accuracy 53.18 %\n",
      "Loss: 1.338556\n",
      "Test Accuracy 52.94 %\n",
      "Test Loss: 1.376449\n",
      "Accuracy 53.16 %\n",
      "Loss: 1.343432\n",
      "Test Accuracy 52.73 %\n",
      "Test Loss: 1.364215\n",
      "Accuracy 53.33 %\n",
      "Loss: 1.323867\n",
      "Test Accuracy 52.91 %\n",
      "Test Loss: 1.358121\n",
      "Accuracy 53.72 %\n",
      "Loss: 1.320342\n",
      "Test Accuracy 52.25 %\n",
      "Test Loss: 1.373140\n",
      "Accuracy 53.57 %\n",
      "Loss: 1.321273\n",
      "Test Accuracy 52.29 %\n",
      "Test Loss: 1.371943\n",
      "Accuracy 53.91 %\n",
      "Loss: 1.313681\n",
      "Test Accuracy 53.05 %\n",
      "Test Loss: 1.382589\n",
      "Accuracy 54.02 %\n",
      "Loss: 1.302297\n",
      "Test Accuracy 52.51 %\n",
      "Test Loss: 1.368327\n",
      "Accuracy 54.04 %\n",
      "Loss: 1.285711\n",
      "Test Accuracy 53.27 %\n",
      "Test Loss: 1.389632\n",
      "Accuracy 54.34 %\n",
      "Loss: 1.282982\n",
      "Test Accuracy 52.80 %\n",
      "Test Loss: 1.364152\n",
      "Accuracy 54.66 %\n",
      "Loss: 1.263492\n",
      "Test Accuracy 51.20 %\n",
      "Test Loss: 1.386889\n",
      "Accuracy 55.00 %\n",
      "Loss: 1.252189\n",
      "Test Accuracy 52.40 %\n",
      "Test Loss: 1.376874\n",
      "Accuracy 55.11 %\n",
      "Loss: 1.231957\n",
      "Test Accuracy 52.94 %\n",
      "Test Loss: 1.384404\n",
      "Accuracy 55.81 %\n",
      "Loss: 1.215005\n",
      "Test Accuracy 51.13 %\n",
      "Test Loss: 1.394724\n",
      "Accuracy 56.30 %\n",
      "Loss: 1.174724\n",
      "Test Accuracy 49.75 %\n",
      "Test Loss: 1.429568\n"
     ]
    }
   ],
   "source": [
    "Taccuracies, Tlosses = train(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03b28c-c5f5-4083-8679-e2dd796ffa8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
